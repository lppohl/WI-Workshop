---
title: Home
layout: page
---

# Improving Bias Evaluation and Mitigation in LMs with theory and methods from social sciences and economics

{% include figure.html img="OIP.jpg" alt="intro image here" caption="Joseph Weizenbaum" width="75%" %}

The emerging field of Responsible AI research has revealed socio-technical biases in language models, leading to discrimination based on attributes such as ethnicity, gender, and more. As language technologies are increasingly capable for tasks like translation, sorting, and text generation; also social scientists are integrating these tools into their research, which poses risks of biased outcomes. Responsible AI aims to address these biases, however, an expanding body of literature criticizes how discrimination is conceptualized, how bias measurements are operationalized, and the existing bias benchmarks [1]. These issues stem from a lack of genuine interdisciplinary collaboration between NLP practitioners and researchers from various social science disciplines.

In this workshop, we encourage interdisciplinary research through using theories, concepts, and research methods from social sciences to improve the evaluation, analysis, and mitigation of socio-technical bias in language technologies. We also encourage research on problems that might arise from using NLP/LT as tools in social sciences and best practices for social scientists to safely use language technologies as tools in their research.

------

*Location:* [Kassenhalle Weizenbaum Institut](https://www.weizenbaum-institut.de/).
*Date:* March 2025
